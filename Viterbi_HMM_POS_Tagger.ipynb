{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Viterbi HMM POS Tagger.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwQLPiCsEZM4"
      },
      "source": [
        "## 1.Building the Dataset\n",
        "\n",
        "To build a Viterbi Decoder , we need to access a trained corpora. This will help us to calculate the emission and transition probabilities. After importing the relevant libraries, we import the treebank corpora. This is a set of Wall Street Journal Articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjjNYqZbDFnj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4d1c0b4-f1bf-4173-f107-02c5363b716d"
      },
      "source": [
        "import nltk\n",
        "import random, pandas as pd, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import metrics\n",
        "\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "wsj = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TwzRxkl12Y1"
      },
      "source": [
        "Once we split the dataset, we can see how many sentences are there in the training and test set. We can also see, how many words are there in each of them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC-Yt0R7FBTT",
        "outputId": "0253d343-64b1-480f-bf15-fe9c708820c1"
      },
      "source": [
        "#random.seed(0)\n",
        "train_set,test_set=train_test_split(wsj,test_size=0.05,random_state=10)\n",
        "\n",
        "print('length of train set {}'.format(len(train_set)))\n",
        "print('length of test set {}'.format(len(test_set)))\n",
        "\n",
        "train_tagged_words=[tup for sent in train_set for tup in sent]\n",
        "test_tagged_words=[tup for sent in test_set for tup in sent]\n",
        "\n",
        "print('Number of tagged train words {}'.format(len(train_tagged_words)))\n",
        "print('Number of tagged test words {}'.format(len(test_tagged_words)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of train set 3718\n",
            "length of test set 196\n",
            "Number of tagged train words 95440\n",
            "Number of tagged test words 5236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiMPgUB42N3G"
      },
      "source": [
        "## 2.Emission and Transition Probabilities\n",
        "\n",
        "In order to build the Viterbi decoder , we have to calculate the:\n",
        "\n",
        "- Transition Probability for each tag combination. This is the matrix __trans__.\n",
        "- In theory , we could make a matrix for Emission Probability as well, for each tag-word combination.However, there's a reason we won't.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx85-WQlIyYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1857e0bf-ef77-48da-975f-a43b79ecadcd"
      },
      "source": [
        "# Building the vocabulary and list of tags\n",
        "vocab=list(set([word.lower() for word,pos in train_tagged_words]))\n",
        "tags=list(set([pos for word,pos in train_tagged_words]))\n",
        "\n",
        "# Count of unique words and tags\n",
        "all_words=len(vocab)\n",
        "all_tags=len(tags)\n",
        "\n",
        "print('Number of words in Vocabulary {}'.format(all_words))\n",
        "print('Number of tags {}'.format(all_tags))\n",
        "\n",
        "\"\"\"\n",
        "trans is a matrix all tags vs all tags. An element in this matrix gives\n",
        "the probability of a tag (row), given a previous tag ( column ). We set initialize this matrix to 0's\n",
        "\"\"\"\n",
        "\n",
        "trans=np.zeros((all_tags,all_tags))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in Vocabulary 11017\n",
            "Number of tags 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QafhUPfnwxIu"
      },
      "source": [
        "To build a matrix of emission probability , we need two nested loops to reach all the words in the vocab and all the tags. With our vocabulary size of 11017 and 12 tags, this would mean more than 100K iterations. There's a better way to handle that.<br> Note that, we need to calculate the $P(w|t)$ of only those words that we'll encounter in the test set. So, when we traverse the words in the test set, we can calculate the $P(w|t)$ on the fly.<br>\n",
        "\n",
        "We define two functions that will help populate the two matrices we defined above\n",
        "- __word_tag__ generates the count of a word associated with a tag and how many times that tag occured.\n",
        "- __t2_given_t1__ genrates the count of a certain tag t2 occurred after a tag t1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvSfhmbthSzt"
      },
      "source": [
        "def word_tag(this_word,this_tag,tagged_words=train_tagged_words):\n",
        "\n",
        "  #List of tuples where tag matches this_tag\n",
        "  this_word=this_word.lower()\n",
        "  tuples_this_tag=[(word,tag) for word,tag in tagged_words if tag==this_tag]\n",
        "\n",
        "  #List of tuples from tuples_this_tag where word is this_word\n",
        "  word_given_tag=[word for word,tag in tuples_this_tag if word.lower()==this_word]\n",
        "\n",
        "  #Count of this_word passed to the function with this_tag\n",
        "  #and count of total number of times, this_tag occured  \n",
        "  \n",
        "  tag_count=len(tuples_this_tag)\n",
        "  word_count=len(word_given_tag)\n",
        "  \n",
        "  return (word_count,tag_count)\n",
        "\n",
        "def t2_given_t1(t1,t2,tagged_words=train_tagged_words):\n",
        "  \n",
        "  # How many times tag t1 occured in the training set\n",
        "  this_tag=[tag for word,tag in tagged_words if tag==t1]\n",
        "  tag_count=len(this_tag)\n",
        "\n",
        "  # How many time tag t1 was followed by tag t2\n",
        "  t2_count=0\n",
        "  for n in range(len(tagged_words)-1):\n",
        "    if tagged_words[n][1]==t1 and tagged_words[n+1][1]==t2:\n",
        "      t2_count+=1\n",
        "\n",
        "  return (t2_count,tag_count)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA8nroHZ_JES"
      },
      "source": [
        "Let us try both the functions for some example combination. The following code snippet shows that the word __Your__ occured as a pronoun 27 times and there are 2588 Pronouns overall in the training set. Similarly the tag __DET__ was followed by __NOUN__ 5298 times out of 8301 determinants in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z5w4XOrawOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a27549a2-3a74-4b35-ca55-23cd1b3e13d7"
      },
      "source": [
        "print(word_tag('Your','PRON'))\n",
        "print(t2_given_t1('DET','NOUN'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(26, 2604)\n",
            "(5286, 8287)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBTD0aJRowy1"
      },
      "source": [
        "We can use these numbers to calculate the following emission and transition probabilities.<br>\n",
        "$P('Your'|'PRON')=27/2604=0.010368$<br>\n",
        "$P('PRON'|'DET')=5286/8287=0.637866$<br>\n",
        "\n",
        "We can calculate the emission matrix __emsn__ for each possible tag transition. The following code generates a dataframe __tags_df__. Each row in the dataframe is t2 and column is t1. For example, to calculate the probability of __NOUN__ , given the previous tag was __DET__, we locate the __NOUN__ row and __DET__ column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jonsm5Nop53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "573a20d3-0f44-42f2-cf8b-76e2d7dc2693"
      },
      "source": [
        "for ro,t2 in enumerate(tags):\n",
        "  for col,t1 in enumerate(tags):\n",
        "    trans[ro,col]=t2_given_t1(t1,t2)[0]/t2_given_t1(t1,t2)[1]\n",
        "\n",
        "tags_df = pd.DataFrame(trans, columns = tags, index=tags)\n",
        "tags_df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NOUN</th>\n",
              "      <th>CONJ</th>\n",
              "      <th>PRT</th>\n",
              "      <th>.</th>\n",
              "      <th>VERB</th>\n",
              "      <th>X</th>\n",
              "      <th>ADV</th>\n",
              "      <th>PRON</th>\n",
              "      <th>NUM</th>\n",
              "      <th>ADJ</th>\n",
              "      <th>DET</th>\n",
              "      <th>ADP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>NOUN</th>\n",
              "      <td>0.263653</td>\n",
              "      <td>0.350746</td>\n",
              "      <td>0.249674</td>\n",
              "      <td>0.217777</td>\n",
              "      <td>0.110515</td>\n",
              "      <td>0.060863</td>\n",
              "      <td>0.031520</td>\n",
              "      <td>0.209293</td>\n",
              "      <td>0.355193</td>\n",
              "      <td>0.699393</td>\n",
              "      <td>0.637867</td>\n",
              "      <td>0.320990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CONJ</th>\n",
              "      <td>0.042643</td>\n",
              "      <td>0.000466</td>\n",
              "      <td>0.001958</td>\n",
              "      <td>0.058110</td>\n",
              "      <td>0.005207</td>\n",
              "      <td>0.010064</td>\n",
              "      <td>0.007299</td>\n",
              "      <td>0.005376</td>\n",
              "      <td>0.013650</td>\n",
              "      <td>0.017403</td>\n",
              "      <td>0.000483</td>\n",
              "      <td>0.000854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PRT</th>\n",
              "      <td>0.044546</td>\n",
              "      <td>0.004664</td>\n",
              "      <td>0.001958</td>\n",
              "      <td>0.002444</td>\n",
              "      <td>0.031476</td>\n",
              "      <td>0.184824</td>\n",
              "      <td>0.014599</td>\n",
              "      <td>0.012289</td>\n",
              "      <td>0.025816</td>\n",
              "      <td>0.010671</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.001281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>.</th>\n",
              "      <td>0.238470</td>\n",
              "      <td>0.035914</td>\n",
              "      <td>0.042428</td>\n",
              "      <td>0.093682</td>\n",
              "      <td>0.035362</td>\n",
              "      <td>0.162780</td>\n",
              "      <td>0.137027</td>\n",
              "      <td>0.039171</td>\n",
              "      <td>0.118101</td>\n",
              "      <td>0.064029</td>\n",
              "      <td>0.017377</td>\n",
              "      <td>0.039484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VERB</th>\n",
              "      <td>0.146889</td>\n",
              "      <td>0.155317</td>\n",
              "      <td>0.399478</td>\n",
              "      <td>0.088975</td>\n",
              "      <td>0.169193</td>\n",
              "      <td>0.206709</td>\n",
              "      <td>0.345720</td>\n",
              "      <td>0.485407</td>\n",
              "      <td>0.017804</td>\n",
              "      <td>0.011821</td>\n",
              "      <td>0.039218</td>\n",
              "      <td>0.008537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X</th>\n",
              "      <td>0.028807</td>\n",
              "      <td>0.008396</td>\n",
              "      <td>0.012728</td>\n",
              "      <td>0.027697</td>\n",
              "      <td>0.217689</td>\n",
              "      <td>0.074121</td>\n",
              "      <td>0.021566</td>\n",
              "      <td>0.092550</td>\n",
              "      <td>0.208309</td>\n",
              "      <td>0.021015</td>\n",
              "      <td>0.046096</td>\n",
              "      <td>0.034895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ADV</th>\n",
              "      <td>0.016947</td>\n",
              "      <td>0.055037</td>\n",
              "      <td>0.010444</td>\n",
              "      <td>0.054218</td>\n",
              "      <td>0.081293</td>\n",
              "      <td>0.026038</td>\n",
              "      <td>0.078301</td>\n",
              "      <td>0.034562</td>\n",
              "      <td>0.003264</td>\n",
              "      <td>0.004761</td>\n",
              "      <td>0.012188</td>\n",
              "      <td>0.013446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PRON</th>\n",
              "      <td>0.004795</td>\n",
              "      <td>0.059701</td>\n",
              "      <td>0.017624</td>\n",
              "      <td>0.066618</td>\n",
              "      <td>0.036294</td>\n",
              "      <td>0.054952</td>\n",
              "      <td>0.014930</td>\n",
              "      <td>0.007680</td>\n",
              "      <td>0.001187</td>\n",
              "      <td>0.000493</td>\n",
              "      <td>0.003258</td>\n",
              "      <td>0.068829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NUM</th>\n",
              "      <td>0.009810</td>\n",
              "      <td>0.041511</td>\n",
              "      <td>0.054830</td>\n",
              "      <td>0.081644</td>\n",
              "      <td>0.022072</td>\n",
              "      <td>0.002875</td>\n",
              "      <td>0.032847</td>\n",
              "      <td>0.006912</td>\n",
              "      <td>0.184866</td>\n",
              "      <td>0.020850</td>\n",
              "      <td>0.021841</td>\n",
              "      <td>0.063280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ADJ</th>\n",
              "      <td>0.012152</td>\n",
              "      <td>0.113340</td>\n",
              "      <td>0.086162</td>\n",
              "      <td>0.045076</td>\n",
              "      <td>0.064972</td>\n",
              "      <td>0.016773</td>\n",
              "      <td>0.128733</td>\n",
              "      <td>0.073733</td>\n",
              "      <td>0.032938</td>\n",
              "      <td>0.066820</td>\n",
              "      <td>0.206951</td>\n",
              "      <td>0.106712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DET</th>\n",
              "      <td>0.013360</td>\n",
              "      <td>0.121735</td>\n",
              "      <td>0.101501</td>\n",
              "      <td>0.172882</td>\n",
              "      <td>0.134919</td>\n",
              "      <td>0.055272</td>\n",
              "      <td>0.066689</td>\n",
              "      <td>0.009985</td>\n",
              "      <td>0.003264</td>\n",
              "      <td>0.004597</td>\n",
              "      <td>0.005430</td>\n",
              "      <td>0.325045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ADP</th>\n",
              "      <td>0.177928</td>\n",
              "      <td>0.053172</td>\n",
              "      <td>0.021214</td>\n",
              "      <td>0.090786</td>\n",
              "      <td>0.091008</td>\n",
              "      <td>0.144728</td>\n",
              "      <td>0.120770</td>\n",
              "      <td>0.023041</td>\n",
              "      <td>0.035608</td>\n",
              "      <td>0.078148</td>\n",
              "      <td>0.009050</td>\n",
              "      <td>0.016647</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          NOUN      CONJ       PRT  ...       ADJ       DET       ADP\n",
              "NOUN  0.263653  0.350746  0.249674  ...  0.699393  0.637867  0.320990\n",
              "CONJ  0.042643  0.000466  0.001958  ...  0.017403  0.000483  0.000854\n",
              "PRT   0.044546  0.004664  0.001958  ...  0.010671  0.000241  0.001281\n",
              ".     0.238470  0.035914  0.042428  ...  0.064029  0.017377  0.039484\n",
              "VERB  0.146889  0.155317  0.399478  ...  0.011821  0.039218  0.008537\n",
              "X     0.028807  0.008396  0.012728  ...  0.021015  0.046096  0.034895\n",
              "ADV   0.016947  0.055037  0.010444  ...  0.004761  0.012188  0.013446\n",
              "PRON  0.004795  0.059701  0.017624  ...  0.000493  0.003258  0.068829\n",
              "NUM   0.009810  0.041511  0.054830  ...  0.020850  0.021841  0.063280\n",
              "ADJ   0.012152  0.113340  0.086162  ...  0.066820  0.206951  0.106712\n",
              "DET   0.013360  0.121735  0.101501  ...  0.004597  0.005430  0.325045\n",
              "ADP   0.177928  0.053172  0.021214  ...  0.078148  0.009050  0.016647\n",
              "\n",
              "[12 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN73UjZV8viM"
      },
      "source": [
        "## 3.Viterbi Decoder\n",
        "\n",
        "This is the most important part of our Viteri Algorithm. The inner working of the function is as follows.\n",
        "\n",
        "- Pass a list of words from a sentence to the function.\n",
        "- For each word $w$ in the sentence , calculate  $P(w|t_i)*P(t_i|t_{i-1})$ for each of the tags.\n",
        "- Select the index of the maximum product and use it to point at the __tags__ list to predict a tag for the word $w$.\n",
        "- The predicted tag is stored in a list __tag_seq__. This is used to fetch the previous tag, unless we are the beginning of a sentence.\n",
        "- If , we are the beginning of a sentence, the previous tag is \".\" (start tag).\n",
        "- The function returns a list of tuple, with each tuple containing the word and the predicted tag.\n",
        "- In case , we do not find a word in the vocabulary, we simply use a tag with maximum transition probability.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXth296lvcMb"
      },
      "source": [
        "def Viterbi(words,vocab=vocab,tags_df=tags_df):\n",
        "  #An empty list to hold the predicted tags\n",
        "  tag_seq=[]\n",
        "\n",
        "  #Run through each tuple in the sentence. Each tuple has a word and a tag\n",
        "  for n,word in enumerate(words):\n",
        "\n",
        "  #Initialize a list to hold the product of emission and transition\n",
        "  #probabilities \n",
        "\n",
        "    argmax=[]\n",
        "    if word in vocab:\n",
        "  #For each of the tags we calculate the product of transition and emission probabilities\n",
        "  # and append it to argmax \n",
        "      for tag in tags:\n",
        "        if n==0:\n",
        "          #transition probability\n",
        "          trans_p=tags_df.loc[tag,'.']\n",
        "        else:\n",
        "          trans_p=tags_df.loc[tag,tag_seq[n-1]]\n",
        "\n",
        "        emisn_p=word_tag(word,tag)[0]/word_tag(word,tag)[1]\n",
        "        argmax.append(trans_p*emisn_p)\n",
        "\n",
        "  #argmax has now a product for each tag. Select the index of the maximum product\n",
        "  #This index is the predicted tag for current word. \n",
        "      max_p=max(argmax)\n",
        "      max_p_tag_idx=argmax.index(max_p)\n",
        "      tag_seq.append(tags[max_p_tag_idx])\n",
        "    \n",
        "  #If this word from the test set is not in the Vocabulary, then we just use the transition \n",
        "  #probability to predict a tag.\n",
        "    else:\n",
        "      if len(tag_seq)==0:\n",
        "        prev_tag='.'\n",
        "      else:\n",
        "        prev_tag=tag_seq[n-1]\n",
        "      \n",
        "      argmax=list(tags_df.loc[::,'.'].values)\n",
        "      max_p=max(argmax)\n",
        "      max_p_tag_idx=argmax.index(max_p)\n",
        "      tag_seq.append(tags[max_p_tag_idx])\n",
        "  \n",
        "  return list(zip(words,tag_seq))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkJzoYL-_Fkb"
      },
      "source": [
        "## 4.Prediction and Results\n",
        "\n",
        "Now, we simply take the test set and strip the true tags from each word. We pass the resulting list of words to the Viterbi function and store the predictions in __pred_set__   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AAUA0jl1-rk"
      },
      "source": [
        "pred_set=[Viterbi([word for word,pos in sent]) for sent in test_set]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCBGWh91_xp4"
      },
      "source": [
        "The performance of the Viterbi Algorithm can be assessed by comparing __pred_set__ and __test_set__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wosu6KG23Xd5",
        "outputId": "71a9f9f8-bf86-41e5-d9bb-4c6baeaa16c5"
      },
      "source": [
        "pred_tags=[[pos for word,pos in sent] for sent in pred_set]\n",
        "true_tags=[[pos for word,pos in sent] for sent in test_set]\n",
        "\n",
        "print(metrics.flat_classification_report(true_tags, pred_tags, labels=tags, digits=2))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        NOUN       0.75      0.98      0.85      1547\n",
            "        CONJ       0.99      0.87      0.93       121\n",
            "         PRT       0.99      0.97      0.98       155\n",
            "           .       1.00      0.99      0.99       667\n",
            "        VERB       0.96      0.87      0.91       697\n",
            "           X       1.00      0.56      0.72       353\n",
            "         ADV       0.89      0.80      0.84       157\n",
            "        PRON       1.00      0.92      0.96       133\n",
            "         NUM       0.99      0.82      0.89       176\n",
            "         ADJ       0.88      0.64      0.74       306\n",
            "         DET       0.95      0.84      0.89       438\n",
            "         ADP       0.98      0.88      0.93       486\n",
            "\n",
            "    accuracy                           0.88      5236\n",
            "   macro avg       0.95      0.84      0.89      5236\n",
            "weighted avg       0.90      0.88      0.88      5236\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}